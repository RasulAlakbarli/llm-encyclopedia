# Transformers: From Scratch to Understanding

A learning repository exploring the foundational concepts of Large Language Models (LLMs), starting with the original Transformer architecture.

## Topics Covered

### Tokenization
- [BPE Tokenizer](./tokenizers/)

### Positional Encoding
- [Sinusoidal Positional Encoding](./positional_encoders/)
- [Rotary Positional Encoding (RoPE)](./positional_encoders/)

### Attention Mechanisms
- [Scaled Dot-Product Attention](./attentions/)
- [Multi-Head Attention](./attentions/)
- [Group Query Attention (GQA)](./attentions/)

### Models
- [Transformer](./models/)
- [GPT-2](./models/)
- [Mixture of Experts (MoE)](./models/)

### Sampling Strategies
- [Top-K Sampling](./samplers/)
- [Top-P (Nucleus) Sampling](./samplers/)

### Training
- [Training Scripts](./training/)

## Blog

For detailed explanations with visuals, check out the accompanying blog posts:

- [Coming Soon] - <Blog link>

## Resources

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Paper
- [Andrej Karpathy's Github](https://github.com/karpathy) - Check minGPT, nanoGPT, minbpe repos
- [Andrej Karpathy's YouTube](https://www.youtube.com/@AndrejKarpathy) - Check videos about GPT tokenizer and GPT model
- [Medium Post by Sumith Madupu](https://medium.com/@sumith.madupu123/understanding-transformer-architecture-using-simple-math-be6c2e1cdcc7) - A great post explaining transformer architecture with simple math.
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2 Paper
- [Gemini Pro](https://gemini.google.com/) - Very helpful for understanding the concepts and math behind LLMs.